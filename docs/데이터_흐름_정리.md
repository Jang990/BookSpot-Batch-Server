## 도서관 Job
목표: 정보나루 지원 도서관 가져오기

beforeJob : 정보나루에서 도서관 Excel 파일 다운로드
afterJob : 도서관 Excel 파일 제거

- 도서관 Excel 파일 정보를 DB에 저장
- 정보나루 디테일 페이지로 이동할 수 있는 코드 크롤링
    - [정보나루 도서관 페이지](https://www.data4library.kr/libDataL) 크롤링
    - 도서관 이름과 주소가 일치하는 도서관의 naru_detail 필드 업데이트 
- 지원이 종료된 도서관 제거 (미구현) 

## 도서관 도서 재고 파일 Job
- 도서관 재고 CSV 파일 다운로드(naru_detail을 활용해서 크롤링)
    - 1월 기준. 1500개의 파일. 17.5GB의 전체 파일 크기

## 도서 정보 동기화 Job
목표: 전체 재고 파일에서 DB에 없는 유니크한 책 데이터 추가

beforeJob: DB에 있는 ISBN 정보를 메모리로 불러오기
afterJob: 도서 정보 인메모리 clearAll()

싱글 스레드 진행. 
=> 멀티 스레드처리 시 Insert Ignore에서 충돌이 발생하면 supremum record lock이 발생하고 이로 인해 데드락 발생
=> 멀티 스레드로 전환하고 싶다면 애플리케이션 레벨에서 중복 isbn 제어 필요 

- 도서관 재고 정보 (1500개) 파일 읽기
  - 도서 정보(isbn13, 도서명, 저자 등등) 파싱
  - 메모리에 존재하는 ISBN 필터링 
  - INSERT IGNORE + ISBN 메모리 업데이트

## 도서 대출 수 종합 Job
목표: 전체 재고 파일에 ISBN별로 대출 수를 종합하기

멀티 스레드 진행
=> beforeStep 에서 싱글스레드로 미리 HashMap을 세팅하고 put을 사용하지 않음.
=> 그래서 단순한 방법인 HashMap과 AtomicInteger의 조합으로 가능하다고 판단됨.

- beforeJob: isbn에 따른 HashMap<Long, AtomicInteger> 세팅
  - 도서관 재고 정보 파일(1500개) 파일 정보의 ISBN과 LOAN_COUNT를 메모리에 저장
  - 인메모리에 저장한 정보를 파일로 저장
- afterJob: ConcurrentHashMap clearAll();

## 도서 대출 수 반영 Job
목표: 도서 대출 수 종합 Job에서 저장한 파일을 DB에 반영

- 대출 횟수 반영

## 도서관 도서 재고 Job
목표: 도서관 재고 반영

멀티스레드 진행
=> 독립된 도서관 파일을 병렬 처리하는 것이기 때문에 동일한 데이터가 들어갈 위험이 ~~없다.~~ 있다. (도서관에서 같은 책이 여러권 있을 수 있다.)
=> 기존 데이터를 그대로 사용하면 UNIQUE 제약 조건에 충돌이 발생하고 넥스트 키락으로 데드락이 발생한다.
=> 데이터의 중복을 없애면 멀티스레드로 진행할 수 있다.

- DB에서 ISBN-BOOKID를 가져와서 Map에 세팅
- csv 파일에서 필요한 정보만 빼서 csv 파일 생성(book_id, library_id)
- Map 초기화
- 하나의 도서관 파일에서 유니크한 book_id만을 가지고 있는 csv 파일 생성

- 임시 테이블 생성
- 임시 테이블에 `Load Data Infile`을 통해 데이터 삽입
- 원본 테이블에 존재하지 않는 데이터를 삽입
- 원본 테이블에 존재하는 데이터의 `updated_at`을 이번 달로 변경
- 임시 테이블 제거
- `updated_at` 필드가 업데이트 되지 않은 stock 정보 제거 


### 나중에 해야할 일 - 도서관 Job
1. Step - 지원이 종료된 도서관 제거
    - 도서관 재고 정보도 청크 단위로 제거해야한다.
    - 이후 제거된 도서관 정보는 제거
2. Reader에 선언된 페이징 관련 필드들을 ExecutionContext로 넣어야 하지 않을까?
    - 재시작을 한다면 실패한 부분에서 재시작하기 위해?
3. 오류가 발생했을 때 어떻게 할 것인가?

### 나중에 해야할 일 - 도서 Job
소장 도서 파일이 없는 경우가 있다. -> 사용자들이 혼란스럽지 않게 도서관 정보 자체를 제거할 것인가?
```
2025-01-22T16:03:03.680+09:00  WARN 16704 --- [bookspot-batch] [           main] c.b.b.s.p.file.StockFileInfoParser       : 도서관 CSV 파일을 찾을 수 없음. CurrentLibrary[libraryCode=144069, naruDetail=9205, stockUpdatedAt=null]
2025-01-22T16:05:57.753+09:00  WARN 16704 --- [bookspot-batch] [           main] c.b.b.s.p.file.StockFileInfoParser       : 도서관 CSV 파일을 찾을 수 없음. CurrentLibrary[libraryCode=144028, naruDetail=34200, stockUpdatedAt=null]
2025-01-22T16:15:56.436+09:00  WARN 16704 --- [bookspot-batch] [           main] c.b.b.s.p.file.StockFileInfoParser       : 도서관 CSV 파일을 찾을 수 없음. CurrentLibrary[libraryCode=148321, naruDetail=12307, stockUpdatedAt=null]
...
```

## 데이터 옵션
- [정보나루 API](https://data4library.kr/apiUtilization) : 일일 30,000건 제한
- [알라딘 API](https://blog.aladin.co.kr/openapi) : 일일 5,000건 - 서비스 URL 필요
- [네이버 책 API](https://developers.naver.com/docs/serviceapi/search/book/book.md) : 일일 25,000건
- [카카오 책 API](https://developers.kakao.com/docs/latest/ko/daum-search/dev-guide#search-book) : 일일 총 5만건, API 별 3만건
- [정보나루 최신 소장 도서 CSV 파일](https://data4library.kr/openDataL) : 크롤링 방식
